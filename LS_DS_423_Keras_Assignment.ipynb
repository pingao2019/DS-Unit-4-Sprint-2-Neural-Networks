{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LS_DS_423_Keras_Assignment.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pingao2019/DS-Unit-4-Sprint-2-Neural-Networks/blob/master/LS_DS_423_Keras_Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5-n3Q0DZ3ADi",
        "colab_type": "text"
      },
      "source": [
        "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
        "<br></br>\n",
        "\n",
        "# Neural Network Framework (Keras)\n",
        "\n",
        "## *Data Science Unit 4 Sprint 2 Assignment 3*\n",
        "\n",
        "## Use the Keras Library to build a Multi-Layer Perceptron Model on the Boston Housing dataset\n",
        "\n",
        "- The Boston Housing dataset comes with the Keras library so use Keras to import it into your notebook. \n",
        "- Normalize the data (all features should have roughly the same scale)\n",
        "- Import the type of model and layers that you will need from Keras.\n",
        "- Instantiate a model object and use `model.add()` to add layers to your model\n",
        "- Since this is a regression model you will have a single output node in the final layer.\n",
        "- Use activation functions that are appropriate for this task\n",
        "- Compile your model\n",
        "- Fit your model and report its accuracy in terms of Mean Squared Error\n",
        "- Use the history object that is returned from model.fit to make graphs of the model's loss or train/validation accuracies by epoch. \n",
        "- Run this same data through a linear regression model. Which achieves higher accuracy?\n",
        "- Do a little bit of feature engineering and see how that affects your neural network model. (you will need to change your model to accept more inputs)\n",
        "- After feature engineering, which model sees a greater accuracy boost due to the new features?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4DbEjGRD3ADm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "48ac75b5-bbf4-4b7f-8718-436de31986ae"
      },
      "source": [
        "%pwd"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8NLTAR87uYJ-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "ac39ec72-4b9c-419d-a1f0-b98f7d1a6254"
      },
      "source": [
        "##### Your Code Here #####\n",
        "from tensorflow.keras.datasets import boston_housing\n",
        "\n",
        "(X_train, y_train), (X_test, y_test) = boston_housing.load_data()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/boston_housing.npz\n",
            "57344/57026 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "px_Rk8Wy5meA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 257
        },
        "outputId": "ef19968a-f5e6-4732-bc5b-af3285435405"
      },
      "source": [
        "X_train"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1.23247e+00, 0.00000e+00, 8.14000e+00, ..., 2.10000e+01,\n",
              "        3.96900e+02, 1.87200e+01],\n",
              "       [2.17700e-02, 8.25000e+01, 2.03000e+00, ..., 1.47000e+01,\n",
              "        3.95380e+02, 3.11000e+00],\n",
              "       [4.89822e+00, 0.00000e+00, 1.81000e+01, ..., 2.02000e+01,\n",
              "        3.75520e+02, 3.26000e+00],\n",
              "       ...,\n",
              "       [3.46600e-02, 3.50000e+01, 6.06000e+00, ..., 1.69000e+01,\n",
              "        3.62250e+02, 7.83000e+00],\n",
              "       [2.14918e+00, 0.00000e+00, 1.95800e+01, ..., 1.47000e+01,\n",
              "        2.61950e+02, 1.57900e+01],\n",
              "       [1.43900e-02, 6.00000e+01, 2.93000e+00, ..., 1.56000e+01,\n",
              "        3.76700e+02, 4.38000e+00]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XpBJrrKe5zyb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "dc312ca1-36b3-4a85-bfb8-80581267ea73"
      },
      "source": [
        "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((404, 13), (102, 13), (404,), (102,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PEblTi6S1MPV",
        "colab_type": "text"
      },
      "source": [
        "So the the number of input neurons equal to input attributes is 13"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zs0HfcVBDh-l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Normalize the data (all features should have roughly the same scale)\n",
        "from sklearn.preprocessing import Normalizer\n",
        "\n",
        "norm= Normalizer()\n",
        "\n",
        "X_train= norm.transform(X_train)\n",
        "X_test= norm.transform(X_test)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U7dJMevTFJ96",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 257
        },
        "outputId": "154d3ebd-e095-4dcc-a21e-a36260e8cc71"
      },
      "source": [
        "X_train"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[2.41189924e-03, 0.00000000e+00, 1.59296858e-02, ...,\n",
              "        4.10962409e-02, 7.76718953e-01, 3.66343633e-02],\n",
              "       [4.07923050e-05, 1.54587284e-01, 3.80378407e-03, ...,\n",
              "        2.75446433e-02, 7.40857215e-01, 5.82747215e-03],\n",
              "       [6.34505528e-03, 0.00000000e+00, 2.34463745e-02, ...,\n",
              "        2.61666721e-02, 4.86441025e-01, 4.22293817e-03],\n",
              "       ...,\n",
              "       [7.29281484e-05, 7.36435428e-02, 1.27508534e-02, ...,\n",
              "        3.55593107e-02, 7.62210668e-01, 1.64751126e-02],\n",
              "       [4.37205159e-03, 0.00000000e+00, 3.98313637e-02, ...,\n",
              "        2.99040371e-02, 5.32881804e-01, 3.21214113e-02],\n",
              "       [3.09311543e-05, 1.28969372e-01, 6.29800433e-03, ...,\n",
              "        3.35320367e-02, 8.09712706e-01, 9.41476414e-03]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "PuCVtZuDsCrP",
        "colab": {}
      },
      "source": [
        "%load_ext tensorboard\n",
        "\n",
        "import os\n",
        "import datetime\n",
        "import tensorflow as tf\n",
        "\n",
        "logdir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4-0g88GiGTB5",
        "colab_type": "text"
      },
      "source": [
        "1.Import the type of model and layers that you will need from Keras.\n",
        "\n",
        "2. Instantiate a model object and use model.add() to add layers to your model\n",
        "\n",
        "3. Since this is a regression model you will have a single output node in the final layer.\n",
        "\n",
        "4. Use activation functions that are appropriate for this task\n",
        "Compile your model.\n",
        "\n",
        "5. Fit your model and report its accuracy in terms of Mean Squared Error"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5sJWVwpl7rRt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7406ed1f-f38d-4fee-d254-0966d03b63df"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Flatten, Dense\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Flatten(input_shape=(13,)))\n",
        "model.add(Dense(15, activation='relu'))\n",
        "model.add(Dense(1))\n",
        "#Dense(1, activation=\"relu\")\n",
        "                    \n",
        "model.compile(loss='mean_squared_error', optimizer='adam', metrics=['mse'])\n",
        "\n",
        "model.fit(X_train, y_train, epochs=200, validation_data=(X_test,y_test))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "13/13 [==============================] - 0s 14ms/step - loss: 598.3846 - mse: 598.3846 - val_loss: 624.8157 - val_mse: 624.8157\n",
            "Epoch 2/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 591.9205 - mse: 591.9205 - val_loss: 618.2950 - val_mse: 618.2950\n",
            "Epoch 3/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 585.7060 - mse: 585.7060 - val_loss: 611.8410 - val_mse: 611.8410\n",
            "Epoch 4/200\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 579.4018 - mse: 579.4018 - val_loss: 605.2594 - val_mse: 605.2594\n",
            "Epoch 5/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 572.7503 - mse: 572.7503 - val_loss: 598.2302 - val_mse: 598.2302\n",
            "Epoch 6/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 565.6554 - mse: 565.6554 - val_loss: 590.4777 - val_mse: 590.4777\n",
            "Epoch 7/200\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 557.8536 - mse: 557.8536 - val_loss: 582.4479 - val_mse: 582.4479\n",
            "Epoch 8/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 549.9064 - mse: 549.9064 - val_loss: 574.0656 - val_mse: 574.0656\n",
            "Epoch 9/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 541.7022 - mse: 541.7022 - val_loss: 565.3978 - val_mse: 565.3978\n",
            "Epoch 10/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 533.1952 - mse: 533.1952 - val_loss: 556.4677 - val_mse: 556.4677\n",
            "Epoch 11/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 524.3976 - mse: 524.3976 - val_loss: 547.2511 - val_mse: 547.2511\n",
            "Epoch 12/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 515.3230 - mse: 515.3230 - val_loss: 537.7955 - val_mse: 537.7955\n",
            "Epoch 13/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 506.0054 - mse: 506.0054 - val_loss: 528.0262 - val_mse: 528.0262\n",
            "Epoch 14/200\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 496.4348 - mse: 496.4348 - val_loss: 517.9792 - val_mse: 517.9792\n",
            "Epoch 15/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 486.6522 - mse: 486.6522 - val_loss: 507.7870 - val_mse: 507.7870\n",
            "Epoch 16/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 476.8170 - mse: 476.8170 - val_loss: 497.5949 - val_mse: 497.5949\n",
            "Epoch 17/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 466.8739 - mse: 466.8739 - val_loss: 487.2920 - val_mse: 487.2920\n",
            "Epoch 18/200\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 456.8540 - mse: 456.8540 - val_loss: 476.6659 - val_mse: 476.6659\n",
            "Epoch 19/200\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 446.3774 - mse: 446.3774 - val_loss: 465.9797 - val_mse: 465.9797\n",
            "Epoch 20/200\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 436.0442 - mse: 436.0442 - val_loss: 454.8528 - val_mse: 454.8528\n",
            "Epoch 21/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 425.2167 - mse: 425.2167 - val_loss: 443.7556 - val_mse: 443.7556\n",
            "Epoch 22/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 414.3242 - mse: 414.3242 - val_loss: 432.5371 - val_mse: 432.5371\n",
            "Epoch 23/200\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 403.4601 - mse: 403.4601 - val_loss: 421.0234 - val_mse: 421.0234\n",
            "Epoch 24/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 392.3100 - mse: 392.3100 - val_loss: 409.5054 - val_mse: 409.5054\n",
            "Epoch 25/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 381.1998 - mse: 381.1998 - val_loss: 397.8585 - val_mse: 397.8585\n",
            "Epoch 26/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 369.9517 - mse: 369.9517 - val_loss: 386.2709 - val_mse: 386.2709\n",
            "Epoch 27/200\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 358.6744 - mse: 358.6744 - val_loss: 374.6707 - val_mse: 374.6707\n",
            "Epoch 28/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 347.6124 - mse: 347.6124 - val_loss: 362.9258 - val_mse: 362.9258\n",
            "Epoch 29/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 336.4372 - mse: 336.4372 - val_loss: 351.3319 - val_mse: 351.3319\n",
            "Epoch 30/200\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 325.1532 - mse: 325.1532 - val_loss: 340.0653 - val_mse: 340.0653\n",
            "Epoch 31/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 314.2487 - mse: 314.2487 - val_loss: 328.6548 - val_mse: 328.6548\n",
            "Epoch 32/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 303.4574 - mse: 303.4574 - val_loss: 317.2995 - val_mse: 317.2995\n",
            "Epoch 33/200\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 292.7827 - mse: 292.7827 - val_loss: 306.0751 - val_mse: 306.0751\n",
            "Epoch 34/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 282.0896 - mse: 282.0896 - val_loss: 295.1891 - val_mse: 295.1891\n",
            "Epoch 35/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 271.7771 - mse: 271.7771 - val_loss: 284.4737 - val_mse: 284.4737\n",
            "Epoch 36/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 261.6222 - mse: 261.6222 - val_loss: 273.9958 - val_mse: 273.9958\n",
            "Epoch 37/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 251.8085 - mse: 251.8085 - val_loss: 263.6602 - val_mse: 263.6602\n",
            "Epoch 38/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 242.1420 - mse: 242.1420 - val_loss: 253.6405 - val_mse: 253.6405\n",
            "Epoch 39/200\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 232.7211 - mse: 232.7211 - val_loss: 243.9508 - val_mse: 243.9508\n",
            "Epoch 40/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 223.6025 - mse: 223.6025 - val_loss: 234.5185 - val_mse: 234.5185\n",
            "Epoch 41/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 214.9001 - mse: 214.9001 - val_loss: 225.2690 - val_mse: 225.2690\n",
            "Epoch 42/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 206.2652 - mse: 206.2652 - val_loss: 216.5560 - val_mse: 216.5560\n",
            "Epoch 43/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 198.1325 - mse: 198.1325 - val_loss: 208.0087 - val_mse: 208.0087\n",
            "Epoch 44/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 190.3100 - mse: 190.3100 - val_loss: 199.8468 - val_mse: 199.8468\n",
            "Epoch 45/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 182.7480 - mse: 182.7480 - val_loss: 192.0364 - val_mse: 192.0364\n",
            "Epoch 46/200\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 175.6036 - mse: 175.6036 - val_loss: 184.5677 - val_mse: 184.5677\n",
            "Epoch 47/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 168.7270 - mse: 168.7270 - val_loss: 177.5110 - val_mse: 177.5110\n",
            "Epoch 48/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 162.2937 - mse: 162.2937 - val_loss: 170.7266 - val_mse: 170.7266\n",
            "Epoch 49/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 156.1830 - mse: 156.1830 - val_loss: 164.2905 - val_mse: 164.2905\n",
            "Epoch 50/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 150.3574 - mse: 150.3574 - val_loss: 158.3071 - val_mse: 158.3071\n",
            "Epoch 51/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 144.9971 - mse: 144.9971 - val_loss: 152.5785 - val_mse: 152.5785\n",
            "Epoch 52/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 139.9001 - mse: 139.9001 - val_loss: 147.2097 - val_mse: 147.2097\n",
            "Epoch 53/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 135.1753 - mse: 135.1753 - val_loss: 142.1357 - val_mse: 142.1357\n",
            "Epoch 54/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 130.7565 - mse: 130.7565 - val_loss: 137.4262 - val_mse: 137.4262\n",
            "Epoch 55/200\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 126.5988 - mse: 126.5988 - val_loss: 133.1197 - val_mse: 133.1197\n",
            "Epoch 56/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 122.8230 - mse: 122.8230 - val_loss: 128.9842 - val_mse: 128.9842\n",
            "Epoch 57/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 119.1834 - mse: 119.1834 - val_loss: 125.2735 - val_mse: 125.2735\n",
            "Epoch 58/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 115.8817 - mse: 115.8817 - val_loss: 121.7653 - val_mse: 121.7653\n",
            "Epoch 59/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 112.8312 - mse: 112.8312 - val_loss: 118.5003 - val_mse: 118.5003\n",
            "Epoch 60/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 110.1468 - mse: 110.1468 - val_loss: 115.3772 - val_mse: 115.3772\n",
            "Epoch 61/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 107.5320 - mse: 107.5320 - val_loss: 112.6710 - val_mse: 112.6710\n",
            "Epoch 62/200\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 105.2387 - mse: 105.2387 - val_loss: 110.1365 - val_mse: 110.1365\n",
            "Epoch 63/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 103.1144 - mse: 103.1144 - val_loss: 107.8046 - val_mse: 107.8046\n",
            "Epoch 64/200\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 101.2443 - mse: 101.2443 - val_loss: 105.6057 - val_mse: 105.6057\n",
            "Epoch 65/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 99.4508 - mse: 99.4508 - val_loss: 103.6744 - val_mse: 103.6744\n",
            "Epoch 66/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 97.8344 - mse: 97.8344 - val_loss: 101.9777 - val_mse: 101.9777\n",
            "Epoch 67/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 96.4875 - mse: 96.4875 - val_loss: 100.2955 - val_mse: 100.2955\n",
            "Epoch 68/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 95.1074 - mse: 95.1074 - val_loss: 98.9121 - val_mse: 98.9121\n",
            "Epoch 69/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 93.9682 - mse: 93.9682 - val_loss: 97.5634 - val_mse: 97.5634\n",
            "Epoch 70/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 92.9280 - mse: 92.9280 - val_loss: 96.2897 - val_mse: 96.2897\n",
            "Epoch 71/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 91.9692 - mse: 91.9692 - val_loss: 95.1586 - val_mse: 95.1586\n",
            "Epoch 72/200\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 91.1117 - mse: 91.1117 - val_loss: 94.1633 - val_mse: 94.1633\n",
            "Epoch 73/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 90.3647 - mse: 90.3647 - val_loss: 93.2535 - val_mse: 93.2535\n",
            "Epoch 74/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 89.6575 - mse: 89.6575 - val_loss: 92.4973 - val_mse: 92.4973\n",
            "Epoch 75/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 89.1337 - mse: 89.1337 - val_loss: 91.6546 - val_mse: 91.6546\n",
            "Epoch 76/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 88.5481 - mse: 88.5481 - val_loss: 90.9737 - val_mse: 90.9737\n",
            "Epoch 77/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 88.0416 - mse: 88.0416 - val_loss: 90.3698 - val_mse: 90.3698\n",
            "Epoch 78/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 87.5910 - mse: 87.5910 - val_loss: 89.8089 - val_mse: 89.8089\n",
            "Epoch 79/200\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 87.1653 - mse: 87.1653 - val_loss: 89.2880 - val_mse: 89.2880\n",
            "Epoch 80/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 86.7779 - mse: 86.7779 - val_loss: 88.7918 - val_mse: 88.7918\n",
            "Epoch 81/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 86.4067 - mse: 86.4067 - val_loss: 88.3227 - val_mse: 88.3227\n",
            "Epoch 82/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 86.0448 - mse: 86.0448 - val_loss: 87.8343 - val_mse: 87.8343\n",
            "Epoch 83/200\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 85.7103 - mse: 85.7103 - val_loss: 87.3812 - val_mse: 87.3812\n",
            "Epoch 84/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 85.3811 - mse: 85.3811 - val_loss: 86.9902 - val_mse: 86.9902\n",
            "Epoch 85/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 85.0804 - mse: 85.0804 - val_loss: 86.6372 - val_mse: 86.6372\n",
            "Epoch 86/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 84.7902 - mse: 84.7902 - val_loss: 86.3226 - val_mse: 86.3226\n",
            "Epoch 87/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 84.5196 - mse: 84.5196 - val_loss: 85.9496 - val_mse: 85.9496\n",
            "Epoch 88/200\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 84.2318 - mse: 84.2318 - val_loss: 85.6239 - val_mse: 85.6239\n",
            "Epoch 89/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 83.9746 - mse: 83.9746 - val_loss: 85.2466 - val_mse: 85.2466\n",
            "Epoch 90/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 83.6921 - mse: 83.6921 - val_loss: 84.9240 - val_mse: 84.9240\n",
            "Epoch 91/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 83.4363 - mse: 83.4363 - val_loss: 84.6279 - val_mse: 84.6279\n",
            "Epoch 92/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 83.1847 - mse: 83.1847 - val_loss: 84.2981 - val_mse: 84.2981\n",
            "Epoch 93/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 82.9172 - mse: 82.9172 - val_loss: 84.0013 - val_mse: 84.0013\n",
            "Epoch 94/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 82.6576 - mse: 82.6576 - val_loss: 83.7328 - val_mse: 83.7328\n",
            "Epoch 95/200\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 82.4130 - mse: 82.4130 - val_loss: 83.4347 - val_mse: 83.4347\n",
            "Epoch 96/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 82.1530 - mse: 82.1530 - val_loss: 83.1564 - val_mse: 83.1564\n",
            "Epoch 97/200\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 81.9126 - mse: 81.9126 - val_loss: 82.8817 - val_mse: 82.8817\n",
            "Epoch 98/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 81.6500 - mse: 81.6500 - val_loss: 82.6250 - val_mse: 82.6250\n",
            "Epoch 99/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 81.3988 - mse: 81.3988 - val_loss: 82.3600 - val_mse: 82.3600\n",
            "Epoch 100/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 81.1546 - mse: 81.1546 - val_loss: 82.0698 - val_mse: 82.0698\n",
            "Epoch 101/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 80.8949 - mse: 80.8949 - val_loss: 81.8270 - val_mse: 81.8270\n",
            "Epoch 102/200\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 80.6371 - mse: 80.6371 - val_loss: 81.5279 - val_mse: 81.5279\n",
            "Epoch 103/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 80.3718 - mse: 80.3718 - val_loss: 81.2798 - val_mse: 81.2798\n",
            "Epoch 104/200\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 80.1097 - mse: 80.1097 - val_loss: 80.9927 - val_mse: 80.9927\n",
            "Epoch 105/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 79.8457 - mse: 79.8457 - val_loss: 80.7133 - val_mse: 80.7133\n",
            "Epoch 106/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 79.5919 - mse: 79.5919 - val_loss: 80.4442 - val_mse: 80.4442\n",
            "Epoch 107/200\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 79.3458 - mse: 79.3458 - val_loss: 80.1292 - val_mse: 80.1292\n",
            "Epoch 108/200\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 79.0487 - mse: 79.0487 - val_loss: 79.9020 - val_mse: 79.9020\n",
            "Epoch 109/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 78.7929 - mse: 78.7929 - val_loss: 79.6391 - val_mse: 79.6391\n",
            "Epoch 110/200\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 78.5256 - mse: 78.5256 - val_loss: 79.2947 - val_mse: 79.2947\n",
            "Epoch 111/200\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 78.2344 - mse: 78.2344 - val_loss: 79.0138 - val_mse: 79.0138\n",
            "Epoch 112/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 77.9741 - mse: 77.9741 - val_loss: 78.7326 - val_mse: 78.7326\n",
            "Epoch 113/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 77.6983 - mse: 77.6983 - val_loss: 78.3993 - val_mse: 78.3993\n",
            "Epoch 114/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 77.4373 - mse: 77.4373 - val_loss: 78.1510 - val_mse: 78.1510\n",
            "Epoch 115/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 77.1429 - mse: 77.1429 - val_loss: 77.8612 - val_mse: 77.8612\n",
            "Epoch 116/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 76.8889 - mse: 76.8889 - val_loss: 77.5308 - val_mse: 77.5308\n",
            "Epoch 117/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 76.5835 - mse: 76.5835 - val_loss: 77.2803 - val_mse: 77.2803\n",
            "Epoch 118/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 76.3240 - mse: 76.3240 - val_loss: 77.0144 - val_mse: 77.0144\n",
            "Epoch 119/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 76.0439 - mse: 76.0439 - val_loss: 76.7146 - val_mse: 76.7146\n",
            "Epoch 120/200\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 75.7760 - mse: 75.7760 - val_loss: 76.4191 - val_mse: 76.4191\n",
            "Epoch 121/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 75.4975 - mse: 75.4975 - val_loss: 76.1552 - val_mse: 76.1552\n",
            "Epoch 122/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 75.1969 - mse: 75.1969 - val_loss: 75.8365 - val_mse: 75.8365\n",
            "Epoch 123/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 74.9272 - mse: 74.9272 - val_loss: 75.5242 - val_mse: 75.5242\n",
            "Epoch 124/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 74.6394 - mse: 74.6394 - val_loss: 75.2887 - val_mse: 75.2887\n",
            "Epoch 125/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 74.3699 - mse: 74.3699 - val_loss: 74.9617 - val_mse: 74.9617\n",
            "Epoch 126/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 74.0882 - mse: 74.0882 - val_loss: 74.7108 - val_mse: 74.7108\n",
            "Epoch 127/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 73.8078 - mse: 73.8078 - val_loss: 74.3852 - val_mse: 74.3852\n",
            "Epoch 128/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 73.5575 - mse: 73.5575 - val_loss: 74.1003 - val_mse: 74.1003\n",
            "Epoch 129/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 73.2863 - mse: 73.2863 - val_loss: 73.7786 - val_mse: 73.7786\n",
            "Epoch 130/200\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 73.0298 - mse: 73.0298 - val_loss: 73.5593 - val_mse: 73.5593\n",
            "Epoch 131/200\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 72.7451 - mse: 72.7451 - val_loss: 73.3281 - val_mse: 73.3281\n",
            "Epoch 132/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 72.4975 - mse: 72.4975 - val_loss: 72.9302 - val_mse: 72.9302\n",
            "Epoch 133/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 72.2163 - mse: 72.2163 - val_loss: 72.6775 - val_mse: 72.6775\n",
            "Epoch 134/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 71.9440 - mse: 71.9440 - val_loss: 72.3498 - val_mse: 72.3498\n",
            "Epoch 135/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 71.7106 - mse: 71.7106 - val_loss: 72.1541 - val_mse: 72.1541\n",
            "Epoch 136/200\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 71.4118 - mse: 71.4118 - val_loss: 71.8521 - val_mse: 71.8521\n",
            "Epoch 137/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 71.1637 - mse: 71.1637 - val_loss: 71.5787 - val_mse: 71.5787\n",
            "Epoch 138/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 70.9137 - mse: 70.9137 - val_loss: 71.2829 - val_mse: 71.2829\n",
            "Epoch 139/200\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 70.6558 - mse: 70.6558 - val_loss: 71.0539 - val_mse: 71.0539\n",
            "Epoch 140/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 70.4089 - mse: 70.4089 - val_loss: 70.7952 - val_mse: 70.7952\n",
            "Epoch 141/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 70.1728 - mse: 70.1728 - val_loss: 70.5235 - val_mse: 70.5235\n",
            "Epoch 142/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 69.9423 - mse: 69.9423 - val_loss: 70.2197 - val_mse: 70.2197\n",
            "Epoch 143/200\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 69.7067 - mse: 69.7067 - val_loss: 69.9063 - val_mse: 69.9063\n",
            "Epoch 144/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 69.4579 - mse: 69.4579 - val_loss: 69.7038 - val_mse: 69.7038\n",
            "Epoch 145/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 69.3132 - mse: 69.3132 - val_loss: 69.5527 - val_mse: 69.5527\n",
            "Epoch 146/200\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 68.9787 - mse: 68.9787 - val_loss: 69.2573 - val_mse: 69.2573\n",
            "Epoch 147/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 68.7735 - mse: 68.7735 - val_loss: 68.9622 - val_mse: 68.9622\n",
            "Epoch 148/200\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 68.5506 - mse: 68.5506 - val_loss: 68.6864 - val_mse: 68.6864\n",
            "Epoch 149/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 68.3321 - mse: 68.3321 - val_loss: 68.4651 - val_mse: 68.4651\n",
            "Epoch 150/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 68.1345 - mse: 68.1345 - val_loss: 68.3472 - val_mse: 68.3472\n",
            "Epoch 151/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 67.8947 - mse: 67.8947 - val_loss: 68.0498 - val_mse: 68.0498\n",
            "Epoch 152/200\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 67.6823 - mse: 67.6823 - val_loss: 67.8465 - val_mse: 67.8465\n",
            "Epoch 153/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 67.4891 - mse: 67.4891 - val_loss: 67.6253 - val_mse: 67.6253\n",
            "Epoch 154/200\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 67.2504 - mse: 67.2504 - val_loss: 67.3410 - val_mse: 67.3410\n",
            "Epoch 155/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 67.0501 - mse: 67.0501 - val_loss: 67.1366 - val_mse: 67.1366\n",
            "Epoch 156/200\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 66.8671 - mse: 66.8671 - val_loss: 66.8601 - val_mse: 66.8601\n",
            "Epoch 157/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 66.6576 - mse: 66.6576 - val_loss: 66.6706 - val_mse: 66.6706\n",
            "Epoch 158/200\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 66.4647 - mse: 66.4647 - val_loss: 66.4898 - val_mse: 66.4898\n",
            "Epoch 159/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 66.2745 - mse: 66.2745 - val_loss: 66.2814 - val_mse: 66.2814\n",
            "Epoch 160/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 66.0931 - mse: 66.0931 - val_loss: 66.1076 - val_mse: 66.1076\n",
            "Epoch 161/200\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 65.9007 - mse: 65.9007 - val_loss: 65.8998 - val_mse: 65.8998\n",
            "Epoch 162/200\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 65.7299 - mse: 65.7299 - val_loss: 65.6410 - val_mse: 65.6410\n",
            "Epoch 163/200\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 65.5606 - mse: 65.5606 - val_loss: 65.4851 - val_mse: 65.4851\n",
            "Epoch 164/200\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 65.3828 - mse: 65.3828 - val_loss: 65.3637 - val_mse: 65.3637\n",
            "Epoch 165/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 65.2058 - mse: 65.2058 - val_loss: 65.1434 - val_mse: 65.1434\n",
            "Epoch 166/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 65.0458 - mse: 65.0458 - val_loss: 64.9023 - val_mse: 64.9023\n",
            "Epoch 167/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 64.8726 - mse: 64.8726 - val_loss: 64.7482 - val_mse: 64.7482\n",
            "Epoch 168/200\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 64.7581 - mse: 64.7581 - val_loss: 64.5109 - val_mse: 64.5109\n",
            "Epoch 169/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 64.5906 - mse: 64.5906 - val_loss: 64.5033 - val_mse: 64.5033\n",
            "Epoch 170/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 64.4049 - mse: 64.4049 - val_loss: 64.3335 - val_mse: 64.3335\n",
            "Epoch 171/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 64.2728 - mse: 64.2728 - val_loss: 64.0552 - val_mse: 64.0552\n",
            "Epoch 172/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 64.1090 - mse: 64.1090 - val_loss: 63.8760 - val_mse: 63.8760\n",
            "Epoch 173/200\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 63.9391 - mse: 63.9391 - val_loss: 63.6914 - val_mse: 63.6914\n",
            "Epoch 174/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 63.8135 - mse: 63.8135 - val_loss: 63.5840 - val_mse: 63.5840\n",
            "Epoch 175/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 63.7005 - mse: 63.7005 - val_loss: 63.3423 - val_mse: 63.3423\n",
            "Epoch 176/200\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 63.5341 - mse: 63.5341 - val_loss: 63.2690 - val_mse: 63.2690\n",
            "Epoch 177/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 63.4037 - mse: 63.4037 - val_loss: 63.1613 - val_mse: 63.1613\n",
            "Epoch 178/200\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 63.2655 - mse: 63.2655 - val_loss: 63.0268 - val_mse: 63.0268\n",
            "Epoch 179/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 63.1351 - mse: 63.1351 - val_loss: 62.8820 - val_mse: 62.8820\n",
            "Epoch 180/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 63.0155 - mse: 63.0155 - val_loss: 62.7389 - val_mse: 62.7389\n",
            "Epoch 181/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 62.9001 - mse: 62.9001 - val_loss: 62.5843 - val_mse: 62.5843\n",
            "Epoch 182/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 62.7667 - mse: 62.7667 - val_loss: 62.4334 - val_mse: 62.4334\n",
            "Epoch 183/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 62.6542 - mse: 62.6542 - val_loss: 62.3130 - val_mse: 62.3130\n",
            "Epoch 184/200\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 62.5336 - mse: 62.5336 - val_loss: 62.1943 - val_mse: 62.1943\n",
            "Epoch 185/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 62.4093 - mse: 62.4093 - val_loss: 62.0967 - val_mse: 62.0967\n",
            "Epoch 186/200\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 62.3102 - mse: 62.3102 - val_loss: 61.9452 - val_mse: 61.9452\n",
            "Epoch 187/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 62.2018 - mse: 62.2018 - val_loss: 61.8715 - val_mse: 61.8715\n",
            "Epoch 188/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 62.0907 - mse: 62.0907 - val_loss: 61.7604 - val_mse: 61.7604\n",
            "Epoch 189/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 61.9662 - mse: 61.9662 - val_loss: 61.5904 - val_mse: 61.5904\n",
            "Epoch 190/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 61.8919 - mse: 61.8919 - val_loss: 61.3803 - val_mse: 61.3803\n",
            "Epoch 191/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 61.7681 - mse: 61.7681 - val_loss: 61.3180 - val_mse: 61.3180\n",
            "Epoch 192/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 61.6606 - mse: 61.6606 - val_loss: 61.2410 - val_mse: 61.2410\n",
            "Epoch 193/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 61.5788 - mse: 61.5788 - val_loss: 61.1002 - val_mse: 61.1002\n",
            "Epoch 194/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 61.4847 - mse: 61.4847 - val_loss: 60.9986 - val_mse: 60.9986\n",
            "Epoch 195/200\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 61.3721 - mse: 61.3721 - val_loss: 60.9197 - val_mse: 60.9197\n",
            "Epoch 196/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 61.2920 - mse: 61.2920 - val_loss: 60.8278 - val_mse: 60.8278\n",
            "Epoch 197/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 61.1905 - mse: 61.1905 - val_loss: 60.7135 - val_mse: 60.7135\n",
            "Epoch 198/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 61.0968 - mse: 61.0968 - val_loss: 60.6234 - val_mse: 60.6234\n",
            "Epoch 199/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 61.0263 - mse: 61.0263 - val_loss: 60.4693 - val_mse: 60.4693\n",
            "Epoch 200/200\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 60.9506 - mse: 60.9506 - val_loss: 60.3594 - val_mse: 60.3594\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fec7016f080>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MKXjAVYSP-bE",
        "colab_type": "text"
      },
      "source": [
        "Use the history object that is returned from model.\n",
        "\n",
        "fit to make graphs of the model's loss or train/validation accuracies by epoch.\n",
        "\n",
        "Run this same data through a linear regression model. Which achieves higher accuracy?\n",
        "\n",
        "Do a little bit of feature engineering and see how that affects your neural network model. (you will need to change your model to accept more inputs)\n",
        "\n",
        "After feature engineering, which model sees a greater accuracy boost due to the new features?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "szi6-IpuzaH1",
        "colab": {}
      },
      "source": [
        "#from keras.callbacks import History \n",
        "#history = History()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "We8wtA4bTyqs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#model.fit(X_train, y_train, nb_epoch=4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IVuzOK6hUcka",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "533c7c78-6c3d-472f-96ce-9ca1709f1484"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Flatten, Dense\n",
        "\n",
        "model = Sequential([Flatten(input_shape=(13,)),\n",
        "    Dense(15, activation=\"relu\"),\n",
        "    Dense(1, activation=\"relu\")\n",
        "])\n",
        "                    \n",
        "model.compile(loss='mean_squared_error', optimizer='adam', metrics=['mse'])\n",
        "\n",
        "model.fit(X_train, y_train, epochs=200, validation_data=(X_test,y_test))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "13/13 [==============================] - 0s 9ms/step - loss: 576.4895 - mse: 576.4895 - val_loss: 602.5540 - val_mse: 602.5540\n",
            "Epoch 2/200\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 570.7158 - mse: 570.7158 - val_loss: 596.6092 - val_mse: 596.6092\n",
            "Epoch 3/200\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 564.9441 - mse: 564.9441 - val_loss: 590.2958 - val_mse: 590.2958\n",
            "Epoch 4/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 558.6349 - mse: 558.6349 - val_loss: 583.5803 - val_mse: 583.5803\n",
            "Epoch 5/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 551.8273 - mse: 551.8273 - val_loss: 575.8767 - val_mse: 575.8767\n",
            "Epoch 6/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 543.9373 - mse: 543.9373 - val_loss: 567.3246 - val_mse: 567.3246\n",
            "Epoch 7/200\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 535.3353 - mse: 535.3353 - val_loss: 558.3002 - val_mse: 558.3002\n",
            "Epoch 8/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 526.3367 - mse: 526.3367 - val_loss: 548.7085 - val_mse: 548.7085\n",
            "Epoch 9/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 516.7914 - mse: 516.7914 - val_loss: 538.5706 - val_mse: 538.5706\n",
            "Epoch 10/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 506.6927 - mse: 506.6927 - val_loss: 527.9337 - val_mse: 527.9337\n",
            "Epoch 11/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 496.1749 - mse: 496.1749 - val_loss: 516.6656 - val_mse: 516.6656\n",
            "Epoch 12/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 485.0502 - mse: 485.0502 - val_loss: 504.9604 - val_mse: 504.9604\n",
            "Epoch 13/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 473.5335 - mse: 473.5335 - val_loss: 492.8043 - val_mse: 492.8043\n",
            "Epoch 14/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 461.6184 - mse: 461.6184 - val_loss: 480.1930 - val_mse: 480.1930\n",
            "Epoch 15/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 449.2229 - mse: 449.2229 - val_loss: 467.3053 - val_mse: 467.3053\n",
            "Epoch 16/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 436.6063 - mse: 436.6063 - val_loss: 454.0972 - val_mse: 454.0972\n",
            "Epoch 17/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 423.7530 - mse: 423.7530 - val_loss: 440.5170 - val_mse: 440.5170\n",
            "Epoch 18/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 410.6230 - mse: 410.6230 - val_loss: 426.6816 - val_mse: 426.6816\n",
            "Epoch 19/200\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 397.0840 - mse: 397.0840 - val_loss: 412.9087 - val_mse: 412.9087\n",
            "Epoch 20/200\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 383.6826 - mse: 383.6826 - val_loss: 398.7343 - val_mse: 398.7343\n",
            "Epoch 21/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 370.0085 - mse: 370.0085 - val_loss: 384.5285 - val_mse: 384.5285\n",
            "Epoch 22/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 356.3598 - mse: 356.3598 - val_loss: 370.3409 - val_mse: 370.3409\n",
            "Epoch 23/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 342.6899 - mse: 342.6899 - val_loss: 356.1320 - val_mse: 356.1320\n",
            "Epoch 24/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 329.0375 - mse: 329.0375 - val_loss: 341.8800 - val_mse: 341.8800\n",
            "Epoch 25/200\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 315.4960 - mse: 315.4960 - val_loss: 327.8163 - val_mse: 327.8163\n",
            "Epoch 26/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 302.0581 - mse: 302.0581 - val_loss: 314.0667 - val_mse: 314.0667\n",
            "Epoch 27/200\n",
            "13/13 [==============================] - 0s 13ms/step - loss: 289.0493 - mse: 289.0493 - val_loss: 300.3448 - val_mse: 300.3448\n",
            "Epoch 28/200\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 276.0613 - mse: 276.0613 - val_loss: 286.9600 - val_mse: 286.9600\n",
            "Epoch 29/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 263.3434 - mse: 263.3434 - val_loss: 273.9935 - val_mse: 273.9935\n",
            "Epoch 30/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 251.2632 - mse: 251.2632 - val_loss: 261.1750 - val_mse: 261.1750\n",
            "Epoch 31/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 239.0807 - mse: 239.0807 - val_loss: 249.0028 - val_mse: 249.0028\n",
            "Epoch 32/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 227.6619 - mse: 227.6619 - val_loss: 237.0116 - val_mse: 237.0116\n",
            "Epoch 33/200\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 216.4550 - mse: 216.4550 - val_loss: 225.4678 - val_mse: 225.4678\n",
            "Epoch 34/200\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 205.7515 - mse: 205.7515 - val_loss: 214.4030 - val_mse: 214.4030\n",
            "Epoch 35/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 195.5146 - mse: 195.5146 - val_loss: 203.8648 - val_mse: 203.8648\n",
            "Epoch 36/200\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 185.7738 - mse: 185.7738 - val_loss: 193.7976 - val_mse: 193.7976\n",
            "Epoch 37/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 176.5805 - mse: 176.5805 - val_loss: 184.1314 - val_mse: 184.1314\n",
            "Epoch 38/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 167.7540 - mse: 167.7540 - val_loss: 175.1203 - val_mse: 175.1203\n",
            "Epoch 39/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 159.5806 - mse: 159.5806 - val_loss: 166.5188 - val_mse: 166.5188\n",
            "Epoch 40/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 151.6918 - mse: 151.6918 - val_loss: 158.6354 - val_mse: 158.6354\n",
            "Epoch 41/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 144.5904 - mse: 144.5904 - val_loss: 151.0515 - val_mse: 151.0515\n",
            "Epoch 42/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 138.0138 - mse: 138.0138 - val_loss: 143.8625 - val_mse: 143.8625\n",
            "Epoch 43/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 131.6603 - mse: 131.6603 - val_loss: 137.5269 - val_mse: 137.5269\n",
            "Epoch 44/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 126.0405 - mse: 126.0405 - val_loss: 131.6139 - val_mse: 131.6139\n",
            "Epoch 45/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 120.8374 - mse: 120.8374 - val_loss: 126.2051 - val_mse: 126.2051\n",
            "Epoch 46/200\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 116.0456 - mse: 116.0456 - val_loss: 121.2546 - val_mse: 121.2546\n",
            "Epoch 47/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 111.9337 - mse: 111.9337 - val_loss: 116.4876 - val_mse: 116.4876\n",
            "Epoch 48/200\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 107.8724 - mse: 107.8724 - val_loss: 112.4132 - val_mse: 112.4132\n",
            "Epoch 49/200\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 104.3455 - mse: 104.3455 - val_loss: 108.7571 - val_mse: 108.7571\n",
            "Epoch 50/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 101.3440 - mse: 101.3440 - val_loss: 105.3404 - val_mse: 105.3404\n",
            "Epoch 51/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 98.4770 - mse: 98.4770 - val_loss: 102.3224 - val_mse: 102.3224\n",
            "Epoch 52/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 96.0182 - mse: 96.0182 - val_loss: 99.5206 - val_mse: 99.5206\n",
            "Epoch 53/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 93.8126 - mse: 93.8126 - val_loss: 97.1137 - val_mse: 97.1137\n",
            "Epoch 54/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 91.8254 - mse: 91.8254 - val_loss: 95.0898 - val_mse: 95.0898\n",
            "Epoch 55/200\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 90.2735 - mse: 90.2735 - val_loss: 93.0915 - val_mse: 93.0915\n",
            "Epoch 56/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 88.6684 - mse: 88.6684 - val_loss: 91.4426 - val_mse: 91.4426\n",
            "Epoch 57/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 87.3735 - mse: 87.3735 - val_loss: 89.9374 - val_mse: 89.9374\n",
            "Epoch 58/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 86.2609 - mse: 86.2609 - val_loss: 88.5634 - val_mse: 88.5634\n",
            "Epoch 59/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 85.2090 - mse: 85.2090 - val_loss: 87.4090 - val_mse: 87.4090\n",
            "Epoch 60/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 84.3381 - mse: 84.3381 - val_loss: 86.3987 - val_mse: 86.3987\n",
            "Epoch 61/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 83.6537 - mse: 83.6537 - val_loss: 85.4238 - val_mse: 85.4238\n",
            "Epoch 62/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 82.9344 - mse: 82.9344 - val_loss: 84.6859 - val_mse: 84.6859\n",
            "Epoch 63/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 82.4370 - mse: 82.4370 - val_loss: 83.9315 - val_mse: 83.9315\n",
            "Epoch 64/200\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 81.8977 - mse: 81.8977 - val_loss: 83.3840 - val_mse: 83.3840\n",
            "Epoch 65/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 81.5128 - mse: 81.5128 - val_loss: 82.8268 - val_mse: 82.8268\n",
            "Epoch 66/200\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 81.1485 - mse: 81.1485 - val_loss: 82.2900 - val_mse: 82.2900\n",
            "Epoch 67/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 80.8074 - mse: 80.8074 - val_loss: 81.8265 - val_mse: 81.8265\n",
            "Epoch 68/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 80.4763 - mse: 80.4763 - val_loss: 81.5023 - val_mse: 81.5023\n",
            "Epoch 69/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 80.2354 - mse: 80.2354 - val_loss: 81.1575 - val_mse: 81.1575\n",
            "Epoch 70/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 80.0052 - mse: 80.0052 - val_loss: 80.8023 - val_mse: 80.8023\n",
            "Epoch 71/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 79.7663 - mse: 79.7663 - val_loss: 80.5094 - val_mse: 80.5094\n",
            "Epoch 72/200\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 79.5533 - mse: 79.5533 - val_loss: 80.2074 - val_mse: 80.2074\n",
            "Epoch 73/200\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 79.3423 - mse: 79.3423 - val_loss: 79.9253 - val_mse: 79.9253\n",
            "Epoch 74/200\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 79.1385 - mse: 79.1385 - val_loss: 79.6427 - val_mse: 79.6427\n",
            "Epoch 75/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 78.9327 - mse: 78.9327 - val_loss: 79.3813 - val_mse: 79.3813\n",
            "Epoch 76/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 78.7236 - mse: 78.7236 - val_loss: 79.1495 - val_mse: 79.1495\n",
            "Epoch 77/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 78.5384 - mse: 78.5384 - val_loss: 78.9037 - val_mse: 78.9037\n",
            "Epoch 78/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 78.3441 - mse: 78.3441 - val_loss: 78.6938 - val_mse: 78.6938\n",
            "Epoch 79/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 78.1585 - mse: 78.1585 - val_loss: 78.4879 - val_mse: 78.4879\n",
            "Epoch 80/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 77.9972 - mse: 77.9972 - val_loss: 78.2514 - val_mse: 78.2514\n",
            "Epoch 81/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 77.7946 - mse: 77.7946 - val_loss: 78.0640 - val_mse: 78.0640\n",
            "Epoch 82/200\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 77.6066 - mse: 77.6066 - val_loss: 77.8660 - val_mse: 77.8660\n",
            "Epoch 83/200\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 77.4403 - mse: 77.4403 - val_loss: 77.6776 - val_mse: 77.6776\n",
            "Epoch 84/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 77.2558 - mse: 77.2558 - val_loss: 77.3909 - val_mse: 77.3909\n",
            "Epoch 85/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 77.0441 - mse: 77.0441 - val_loss: 77.1863 - val_mse: 77.1863\n",
            "Epoch 86/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 76.8561 - mse: 76.8561 - val_loss: 76.9906 - val_mse: 76.9906\n",
            "Epoch 87/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 76.6817 - mse: 76.6817 - val_loss: 76.7671 - val_mse: 76.7671\n",
            "Epoch 88/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 76.4839 - mse: 76.4839 - val_loss: 76.5794 - val_mse: 76.5794\n",
            "Epoch 89/200\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 76.2907 - mse: 76.2907 - val_loss: 76.4035 - val_mse: 76.4035\n",
            "Epoch 90/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 76.1216 - mse: 76.1216 - val_loss: 76.2035 - val_mse: 76.2035\n",
            "Epoch 91/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 75.9142 - mse: 75.9142 - val_loss: 76.0300 - val_mse: 76.0300\n",
            "Epoch 92/200\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 75.7275 - mse: 75.7275 - val_loss: 75.8196 - val_mse: 75.8196\n",
            "Epoch 93/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 75.5364 - mse: 75.5364 - val_loss: 75.6418 - val_mse: 75.6418\n",
            "Epoch 94/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 75.3427 - mse: 75.3427 - val_loss: 75.4015 - val_mse: 75.4015\n",
            "Epoch 95/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 75.1488 - mse: 75.1488 - val_loss: 75.2188 - val_mse: 75.2188\n",
            "Epoch 96/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 74.9488 - mse: 74.9488 - val_loss: 75.0395 - val_mse: 75.0395\n",
            "Epoch 97/200\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 74.7719 - mse: 74.7719 - val_loss: 74.8550 - val_mse: 74.8550\n",
            "Epoch 98/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 74.5695 - mse: 74.5695 - val_loss: 74.6277 - val_mse: 74.6277\n",
            "Epoch 99/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 74.3899 - mse: 74.3899 - val_loss: 74.4691 - val_mse: 74.4691\n",
            "Epoch 100/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 74.1849 - mse: 74.1849 - val_loss: 74.2399 - val_mse: 74.2399\n",
            "Epoch 101/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 73.9897 - mse: 73.9897 - val_loss: 74.0458 - val_mse: 74.0458\n",
            "Epoch 102/200\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 73.7943 - mse: 73.7943 - val_loss: 73.8452 - val_mse: 73.8452\n",
            "Epoch 103/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 73.6085 - mse: 73.6085 - val_loss: 73.6736 - val_mse: 73.6736\n",
            "Epoch 104/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 73.4091 - mse: 73.4091 - val_loss: 73.4625 - val_mse: 73.4625\n",
            "Epoch 105/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 73.2328 - mse: 73.2328 - val_loss: 73.2731 - val_mse: 73.2731\n",
            "Epoch 106/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 73.0373 - mse: 73.0373 - val_loss: 73.0662 - val_mse: 73.0662\n",
            "Epoch 107/200\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 72.8458 - mse: 72.8458 - val_loss: 72.8429 - val_mse: 72.8429\n",
            "Epoch 108/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 72.6607 - mse: 72.6607 - val_loss: 72.6152 - val_mse: 72.6152\n",
            "Epoch 109/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 72.4597 - mse: 72.4597 - val_loss: 72.4472 - val_mse: 72.4472\n",
            "Epoch 110/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 72.2890 - mse: 72.2890 - val_loss: 72.2971 - val_mse: 72.2971\n",
            "Epoch 111/200\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 72.0882 - mse: 72.0882 - val_loss: 72.0896 - val_mse: 72.0896\n",
            "Epoch 112/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 71.9103 - mse: 71.9103 - val_loss: 71.8621 - val_mse: 71.8621\n",
            "Epoch 113/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 71.7140 - mse: 71.7140 - val_loss: 71.6697 - val_mse: 71.6697\n",
            "Epoch 114/200\n",
            "13/13 [==============================] - 0s 7ms/step - loss: 71.5260 - mse: 71.5260 - val_loss: 71.5278 - val_mse: 71.5278\n",
            "Epoch 115/200\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 71.3412 - mse: 71.3412 - val_loss: 71.3122 - val_mse: 71.3122\n",
            "Epoch 116/200\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 71.1646 - mse: 71.1646 - val_loss: 71.0624 - val_mse: 71.0624\n",
            "Epoch 117/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 70.9740 - mse: 70.9740 - val_loss: 70.9203 - val_mse: 70.9203\n",
            "Epoch 118/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 70.8019 - mse: 70.8019 - val_loss: 70.7159 - val_mse: 70.7159\n",
            "Epoch 119/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 70.6086 - mse: 70.6086 - val_loss: 70.5593 - val_mse: 70.5593\n",
            "Epoch 120/200\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 70.4544 - mse: 70.4544 - val_loss: 70.3709 - val_mse: 70.3709\n",
            "Epoch 121/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 70.2773 - mse: 70.2773 - val_loss: 70.2395 - val_mse: 70.2395\n",
            "Epoch 122/200\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 70.0919 - mse: 70.0919 - val_loss: 70.0232 - val_mse: 70.0232\n",
            "Epoch 123/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 69.9346 - mse: 69.9346 - val_loss: 69.9005 - val_mse: 69.9005\n",
            "Epoch 124/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 69.7441 - mse: 69.7441 - val_loss: 69.6797 - val_mse: 69.6797\n",
            "Epoch 125/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 69.5724 - mse: 69.5724 - val_loss: 69.4972 - val_mse: 69.4972\n",
            "Epoch 126/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 69.3931 - mse: 69.3931 - val_loss: 69.3338 - val_mse: 69.3338\n",
            "Epoch 127/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 69.2853 - mse: 69.2853 - val_loss: 69.0757 - val_mse: 69.0757\n",
            "Epoch 128/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 69.0669 - mse: 69.0669 - val_loss: 68.9287 - val_mse: 68.9287\n",
            "Epoch 129/200\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 68.8991 - mse: 68.8991 - val_loss: 68.7727 - val_mse: 68.7727\n",
            "Epoch 130/200\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 68.7408 - mse: 68.7408 - val_loss: 68.6215 - val_mse: 68.6215\n",
            "Epoch 131/200\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 68.5790 - mse: 68.5790 - val_loss: 68.4463 - val_mse: 68.4463\n",
            "Epoch 132/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 68.4148 - mse: 68.4148 - val_loss: 68.2507 - val_mse: 68.2507\n",
            "Epoch 133/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 68.2852 - mse: 68.2852 - val_loss: 68.0491 - val_mse: 68.0491\n",
            "Epoch 134/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 68.1148 - mse: 68.1148 - val_loss: 67.9774 - val_mse: 67.9774\n",
            "Epoch 135/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 67.9368 - mse: 67.9368 - val_loss: 67.7924 - val_mse: 67.7924\n",
            "Epoch 136/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 67.8011 - mse: 67.8011 - val_loss: 67.6033 - val_mse: 67.6033\n",
            "Epoch 137/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 67.6396 - mse: 67.6396 - val_loss: 67.4353 - val_mse: 67.4353\n",
            "Epoch 138/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 67.4870 - mse: 67.4870 - val_loss: 67.3067 - val_mse: 67.3067\n",
            "Epoch 139/200\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 67.3321 - mse: 67.3321 - val_loss: 67.1564 - val_mse: 67.1564\n",
            "Epoch 140/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 67.1982 - mse: 67.1982 - val_loss: 67.0378 - val_mse: 67.0378\n",
            "Epoch 141/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 67.0510 - mse: 67.0510 - val_loss: 66.8277 - val_mse: 66.8277\n",
            "Epoch 142/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 66.8979 - mse: 66.8979 - val_loss: 66.6347 - val_mse: 66.6347\n",
            "Epoch 143/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 66.7460 - mse: 66.7460 - val_loss: 66.5171 - val_mse: 66.5171\n",
            "Epoch 144/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 66.6124 - mse: 66.6124 - val_loss: 66.3861 - val_mse: 66.3861\n",
            "Epoch 145/200\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 66.4612 - mse: 66.4612 - val_loss: 66.1828 - val_mse: 66.1828\n",
            "Epoch 146/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 66.3364 - mse: 66.3364 - val_loss: 66.0213 - val_mse: 66.0213\n",
            "Epoch 147/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 66.1815 - mse: 66.1815 - val_loss: 65.9101 - val_mse: 65.9101\n",
            "Epoch 148/200\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 66.0358 - mse: 66.0358 - val_loss: 65.7712 - val_mse: 65.7712\n",
            "Epoch 149/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 65.9092 - mse: 65.9092 - val_loss: 65.5816 - val_mse: 65.5816\n",
            "Epoch 150/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 65.7800 - mse: 65.7800 - val_loss: 65.4242 - val_mse: 65.4242\n",
            "Epoch 151/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 65.6332 - mse: 65.6332 - val_loss: 65.3441 - val_mse: 65.3441\n",
            "Epoch 152/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 65.5055 - mse: 65.5055 - val_loss: 65.1965 - val_mse: 65.1965\n",
            "Epoch 153/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 65.3907 - mse: 65.3907 - val_loss: 65.0595 - val_mse: 65.0595\n",
            "Epoch 154/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 65.2403 - mse: 65.2403 - val_loss: 64.9029 - val_mse: 64.9029\n",
            "Epoch 155/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 65.1178 - mse: 65.1178 - val_loss: 64.7920 - val_mse: 64.7920\n",
            "Epoch 156/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 64.9846 - mse: 64.9846 - val_loss: 64.6544 - val_mse: 64.6544\n",
            "Epoch 157/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 64.8718 - mse: 64.8718 - val_loss: 64.5361 - val_mse: 64.5361\n",
            "Epoch 158/200\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 64.7324 - mse: 64.7324 - val_loss: 64.3922 - val_mse: 64.3922\n",
            "Epoch 159/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 64.6208 - mse: 64.6208 - val_loss: 64.2416 - val_mse: 64.2416\n",
            "Epoch 160/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 64.4942 - mse: 64.4942 - val_loss: 64.1221 - val_mse: 64.1221\n",
            "Epoch 161/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 64.3762 - mse: 64.3762 - val_loss: 64.0179 - val_mse: 64.0179\n",
            "Epoch 162/200\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 64.2610 - mse: 64.2610 - val_loss: 63.9203 - val_mse: 63.9203\n",
            "Epoch 163/200\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 64.1511 - mse: 64.1511 - val_loss: 63.7581 - val_mse: 63.7581\n",
            "Epoch 164/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 64.0265 - mse: 64.0265 - val_loss: 63.6445 - val_mse: 63.6445\n",
            "Epoch 165/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 63.9319 - mse: 63.9319 - val_loss: 63.5624 - val_mse: 63.5624\n",
            "Epoch 166/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 63.8326 - mse: 63.8326 - val_loss: 63.3608 - val_mse: 63.3608\n",
            "Epoch 167/200\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 63.7289 - mse: 63.7289 - val_loss: 63.3156 - val_mse: 63.3156\n",
            "Epoch 168/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 63.5942 - mse: 63.5942 - val_loss: 63.1228 - val_mse: 63.1228\n",
            "Epoch 169/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 63.5131 - mse: 63.5131 - val_loss: 63.0328 - val_mse: 63.0328\n",
            "Epoch 170/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 63.3705 - mse: 63.3705 - val_loss: 62.8876 - val_mse: 62.8876\n",
            "Epoch 171/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 63.3262 - mse: 63.3262 - val_loss: 62.6914 - val_mse: 62.6914\n",
            "Epoch 172/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 63.1864 - mse: 63.1864 - val_loss: 62.6665 - val_mse: 62.6665\n",
            "Epoch 173/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 63.0726 - mse: 63.0726 - val_loss: 62.5798 - val_mse: 62.5798\n",
            "Epoch 174/200\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 63.0009 - mse: 63.0009 - val_loss: 62.5394 - val_mse: 62.5394\n",
            "Epoch 175/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 62.8724 - mse: 62.8724 - val_loss: 62.4402 - val_mse: 62.4402\n",
            "Epoch 176/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 62.7888 - mse: 62.7888 - val_loss: 62.2752 - val_mse: 62.2752\n",
            "Epoch 177/200\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 62.6958 - mse: 62.6958 - val_loss: 62.2436 - val_mse: 62.2436\n",
            "Epoch 178/200\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 62.6082 - mse: 62.6082 - val_loss: 62.0795 - val_mse: 62.0795\n",
            "Epoch 179/200\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 62.4986 - mse: 62.4986 - val_loss: 62.0163 - val_mse: 62.0163\n",
            "Epoch 180/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 62.3959 - mse: 62.3959 - val_loss: 61.9151 - val_mse: 61.9151\n",
            "Epoch 181/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 62.3149 - mse: 62.3149 - val_loss: 61.8689 - val_mse: 61.8689\n",
            "Epoch 182/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 62.2279 - mse: 62.2279 - val_loss: 61.7587 - val_mse: 61.7587\n",
            "Epoch 183/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 62.1244 - mse: 62.1244 - val_loss: 61.6137 - val_mse: 61.6137\n",
            "Epoch 184/200\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 62.0427 - mse: 62.0427 - val_loss: 61.5239 - val_mse: 61.5239\n",
            "Epoch 185/200\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 61.9577 - mse: 61.9577 - val_loss: 61.4183 - val_mse: 61.4183\n",
            "Epoch 186/200\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 61.9097 - mse: 61.9097 - val_loss: 61.3663 - val_mse: 61.3663\n",
            "Epoch 187/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 61.8094 - mse: 61.8094 - val_loss: 61.1992 - val_mse: 61.1992\n",
            "Epoch 188/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 61.7155 - mse: 61.7155 - val_loss: 61.1414 - val_mse: 61.1414\n",
            "Epoch 189/200\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 61.6293 - mse: 61.6293 - val_loss: 61.0242 - val_mse: 61.0242\n",
            "Epoch 190/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 61.5439 - mse: 61.5439 - val_loss: 60.9996 - val_mse: 60.9996\n",
            "Epoch 191/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 61.4946 - mse: 61.4946 - val_loss: 60.8643 - val_mse: 60.8643\n",
            "Epoch 192/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 61.4049 - mse: 61.4049 - val_loss: 60.7882 - val_mse: 60.7882\n",
            "Epoch 193/200\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 61.3190 - mse: 61.3190 - val_loss: 60.7438 - val_mse: 60.7438\n",
            "Epoch 194/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 61.2497 - mse: 61.2497 - val_loss: 60.6554 - val_mse: 60.6554\n",
            "Epoch 195/200\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 61.1633 - mse: 61.1633 - val_loss: 60.6742 - val_mse: 60.6742\n",
            "Epoch 196/200\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 61.1297 - mse: 61.1297 - val_loss: 60.6356 - val_mse: 60.6356\n",
            "Epoch 197/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 61.0294 - mse: 61.0294 - val_loss: 60.5087 - val_mse: 60.5087\n",
            "Epoch 198/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 60.9546 - mse: 60.9546 - val_loss: 60.3778 - val_mse: 60.3778\n",
            "Epoch 199/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 60.8761 - mse: 60.8761 - val_loss: 60.2915 - val_mse: 60.2915\n",
            "Epoch 200/200\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 60.8381 - mse: 60.8381 - val_loss: 60.1507 - val_mse: 60.1507\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fec240e3550>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4PYUuC3vYmzI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#model.fit(X_train, Y_train, nb_epoch=5, batch_size=16, callbacks=[history])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PpA2GrBWYpx_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "69129fd5-041f-4303-8b3a-97504a2ca49b"
      },
      "source": [
        "print (model.history.history)\n",
        "\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'loss': [576.489501953125, 570.7158203125, 564.944091796875, 558.6349487304688, 551.8272705078125, 543.9373168945312, 535.3352661132812, 526.336669921875, 516.7913818359375, 506.69268798828125, 496.17486572265625, 485.0501708984375, 473.53350830078125, 461.6184387207031, 449.2229309082031, 436.6063232421875, 423.7530212402344, 410.62298583984375, 397.0840148925781, 383.6826171875, 370.008544921875, 356.3598327636719, 342.68994140625, 329.0374755859375, 315.4959716796875, 302.0581359863281, 289.0492858886719, 276.0613098144531, 263.3433837890625, 251.26324462890625, 239.08071899414062, 227.66188049316406, 216.4549560546875, 205.7515106201172, 195.51455688476562, 185.7738037109375, 176.58045959472656, 167.75404357910156, 159.58062744140625, 151.6918487548828, 144.5904083251953, 138.01376342773438, 131.66033935546875, 126.04046630859375, 120.8373794555664, 116.04557037353516, 111.93374633789062, 107.87242889404297, 104.34547424316406, 101.3440170288086, 98.47696685791016, 96.0181655883789, 93.8126220703125, 91.8254165649414, 90.2735366821289, 88.66840362548828, 87.37348937988281, 86.2608642578125, 85.2089614868164, 84.33814239501953, 83.65369415283203, 82.93441772460938, 82.43699645996094, 81.89774322509766, 81.51275634765625, 81.1485366821289, 80.807373046875, 80.47628021240234, 80.23540496826172, 80.00523376464844, 79.76628112792969, 79.55330657958984, 79.34226989746094, 79.13846588134766, 78.9327163696289, 78.7236099243164, 78.5383529663086, 78.3440933227539, 78.15846252441406, 77.99716186523438, 77.7945785522461, 77.60658264160156, 77.44031524658203, 77.2558364868164, 77.0440673828125, 76.85607147216797, 76.68170166015625, 76.48394012451172, 76.29073333740234, 76.1215591430664, 75.91421508789062, 75.72745513916016, 75.53636169433594, 75.3426742553711, 75.14884948730469, 74.94879150390625, 74.77192687988281, 74.56954956054688, 74.38987731933594, 74.18494415283203, 73.98966217041016, 73.79425811767578, 73.60845184326172, 73.40912628173828, 73.23275756835938, 73.03732299804688, 72.8458023071289, 72.66065216064453, 72.45973205566406, 72.28897857666016, 72.08820343017578, 71.9102783203125, 71.71400451660156, 71.52595520019531, 71.3411636352539, 71.16455841064453, 70.97396850585938, 70.80186462402344, 70.60860443115234, 70.45443725585938, 70.27729797363281, 70.09187316894531, 69.93464660644531, 69.74407958984375, 69.5723648071289, 69.3930892944336, 69.28532409667969, 69.06692504882812, 68.89909362792969, 68.7408218383789, 68.57904052734375, 68.41477966308594, 68.28521728515625, 68.11479187011719, 67.93683624267578, 67.80110168457031, 67.6396484375, 67.48696899414062, 67.33209228515625, 67.19818878173828, 67.05101013183594, 66.89788055419922, 66.74597930908203, 66.61244201660156, 66.46119689941406, 66.33639526367188, 66.18145751953125, 66.03583526611328, 65.90917205810547, 65.7800064086914, 65.63323211669922, 65.5055160522461, 65.39073181152344, 65.2402572631836, 65.11778259277344, 64.98460388183594, 64.87175750732422, 64.7323989868164, 64.62079620361328, 64.49418640136719, 64.37620544433594, 64.26095581054688, 64.15113830566406, 64.02645111083984, 63.93193435668945, 63.83258056640625, 63.728858947753906, 63.59418487548828, 63.5130500793457, 63.370521545410156, 63.326229095458984, 63.18637466430664, 63.07259750366211, 63.00091552734375, 62.8724250793457, 62.78877258300781, 62.695804595947266, 62.60818099975586, 62.49859619140625, 62.3958854675293, 62.3149299621582, 62.22785186767578, 62.124385833740234, 62.0427131652832, 61.957725524902344, 61.90971755981445, 61.809356689453125, 61.715545654296875, 61.62928009033203, 61.543922424316406, 61.494625091552734, 61.404945373535156, 61.31904220581055, 61.24969482421875, 61.16328430175781, 61.129695892333984, 61.02939224243164, 60.954593658447266, 60.8760986328125, 60.83805847167969], 'mse': [576.489501953125, 570.7158203125, 564.944091796875, 558.6349487304688, 551.8272705078125, 543.9373168945312, 535.3352661132812, 526.336669921875, 516.7913818359375, 506.69268798828125, 496.17486572265625, 485.0501708984375, 473.53350830078125, 461.6184387207031, 449.2229309082031, 436.6063232421875, 423.7530212402344, 410.62298583984375, 397.0840148925781, 383.6826171875, 370.008544921875, 356.3598327636719, 342.68994140625, 329.0374755859375, 315.4959716796875, 302.0581359863281, 289.0492858886719, 276.0613098144531, 263.3433837890625, 251.26324462890625, 239.08071899414062, 227.66188049316406, 216.4549560546875, 205.7515106201172, 195.51455688476562, 185.7738037109375, 176.58045959472656, 167.75404357910156, 159.58062744140625, 151.6918487548828, 144.5904083251953, 138.01376342773438, 131.66033935546875, 126.04046630859375, 120.8373794555664, 116.04557037353516, 111.93374633789062, 107.87242889404297, 104.34547424316406, 101.3440170288086, 98.47696685791016, 96.0181655883789, 93.8126220703125, 91.8254165649414, 90.2735366821289, 88.66840362548828, 87.37348937988281, 86.2608642578125, 85.2089614868164, 84.33814239501953, 83.65369415283203, 82.93441772460938, 82.43699645996094, 81.89774322509766, 81.51275634765625, 81.1485366821289, 80.807373046875, 80.47628021240234, 80.23540496826172, 80.00523376464844, 79.76628112792969, 79.55330657958984, 79.34226989746094, 79.13846588134766, 78.9327163696289, 78.7236099243164, 78.5383529663086, 78.3440933227539, 78.15846252441406, 77.99716186523438, 77.7945785522461, 77.60658264160156, 77.44031524658203, 77.2558364868164, 77.0440673828125, 76.85607147216797, 76.68170166015625, 76.48394012451172, 76.29073333740234, 76.1215591430664, 75.91421508789062, 75.72745513916016, 75.53636169433594, 75.3426742553711, 75.14884948730469, 74.94879150390625, 74.77192687988281, 74.56954956054688, 74.38987731933594, 74.18494415283203, 73.98966217041016, 73.79425811767578, 73.60845184326172, 73.40912628173828, 73.23275756835938, 73.03732299804688, 72.8458023071289, 72.66065216064453, 72.45973205566406, 72.28897857666016, 72.08820343017578, 71.9102783203125, 71.71400451660156, 71.52595520019531, 71.3411636352539, 71.16455841064453, 70.97396850585938, 70.80186462402344, 70.60860443115234, 70.45443725585938, 70.27729797363281, 70.09187316894531, 69.93464660644531, 69.74407958984375, 69.5723648071289, 69.3930892944336, 69.28532409667969, 69.06692504882812, 68.89909362792969, 68.7408218383789, 68.57904052734375, 68.41477966308594, 68.28521728515625, 68.11479187011719, 67.93683624267578, 67.80110168457031, 67.6396484375, 67.48696899414062, 67.33209228515625, 67.19818878173828, 67.05101013183594, 66.89788055419922, 66.74597930908203, 66.61244201660156, 66.46119689941406, 66.33639526367188, 66.18145751953125, 66.03583526611328, 65.90917205810547, 65.7800064086914, 65.63323211669922, 65.5055160522461, 65.39073181152344, 65.2402572631836, 65.11778259277344, 64.98460388183594, 64.87175750732422, 64.7323989868164, 64.62079620361328, 64.49418640136719, 64.37620544433594, 64.26095581054688, 64.15113830566406, 64.02645111083984, 63.93193435668945, 63.83258056640625, 63.728858947753906, 63.59418487548828, 63.5130500793457, 63.370521545410156, 63.326229095458984, 63.18637466430664, 63.07259750366211, 63.00091552734375, 62.8724250793457, 62.78877258300781, 62.695804595947266, 62.60818099975586, 62.49859619140625, 62.3958854675293, 62.3149299621582, 62.22785186767578, 62.124385833740234, 62.0427131652832, 61.957725524902344, 61.90971755981445, 61.809356689453125, 61.715545654296875, 61.62928009033203, 61.543922424316406, 61.494625091552734, 61.404945373535156, 61.31904220581055, 61.24969482421875, 61.16328430175781, 61.129695892333984, 61.02939224243164, 60.954593658447266, 60.8760986328125, 60.83805847167969], 'val_loss': [602.5540161132812, 596.6091918945312, 590.2958374023438, 583.5802612304688, 575.876708984375, 567.3245849609375, 558.3001708984375, 548.70849609375, 538.5706176757812, 527.9337158203125, 516.6656494140625, 504.9603576660156, 492.80426025390625, 480.1930236816406, 467.3052673339844, 454.0971984863281, 440.5169677734375, 426.681640625, 412.9086608886719, 398.7342529296875, 384.5285339355469, 370.34088134765625, 356.1320495605469, 341.8799743652344, 327.8163146972656, 314.066650390625, 300.3447570800781, 286.9599914550781, 273.9934997558594, 261.17498779296875, 249.00277709960938, 237.01156616210938, 225.46783447265625, 214.40296936035156, 203.86477661132812, 193.79757690429688, 184.1313934326172, 175.12033081054688, 166.518798828125, 158.63536071777344, 151.05152893066406, 143.86251831054688, 137.52694702148438, 131.61392211914062, 126.2051010131836, 121.2546157836914, 116.48756408691406, 112.41320037841797, 108.75714111328125, 105.34043884277344, 102.32237243652344, 99.52059173583984, 97.11366271972656, 95.08984375, 93.0915298461914, 91.44261169433594, 89.93741607666016, 88.56336975097656, 87.40899658203125, 86.3986587524414, 85.42375946044922, 84.68588256835938, 83.9314956665039, 83.38398742675781, 82.82677459716797, 82.29000091552734, 81.8265151977539, 81.50225830078125, 81.15745544433594, 80.80230712890625, 80.50942993164062, 80.20738220214844, 79.92534637451172, 79.64268493652344, 79.38127136230469, 79.14945983886719, 78.90369415283203, 78.69381713867188, 78.4878921508789, 78.25141906738281, 78.06403350830078, 77.86603546142578, 77.67759704589844, 77.39088439941406, 77.18634033203125, 76.9906005859375, 76.76713562011719, 76.57937622070312, 76.4034652709961, 76.20347595214844, 76.02999877929688, 75.81957244873047, 75.64178466796875, 75.40145111083984, 75.21881103515625, 75.03954315185547, 74.85496520996094, 74.62767028808594, 74.46907043457031, 74.23985290527344, 74.04584503173828, 73.84524536132812, 73.67359161376953, 73.46247863769531, 73.27313995361328, 73.06620788574219, 72.84291076660156, 72.61520385742188, 72.44715881347656, 72.29707336425781, 72.0895767211914, 71.86210632324219, 71.66969299316406, 71.52783203125, 71.31219482421875, 71.0623550415039, 70.92031860351562, 70.71585845947266, 70.55933380126953, 70.37091064453125, 70.23949432373047, 70.0232162475586, 69.90045166015625, 69.67972564697266, 69.4972152709961, 69.33379364013672, 69.07569122314453, 68.92870330810547, 68.77268981933594, 68.62150573730469, 68.44633483886719, 68.25071716308594, 68.04912567138672, 67.97738647460938, 67.79236602783203, 67.60325622558594, 67.4353256225586, 67.30673217773438, 67.15636444091797, 67.03778076171875, 66.82771301269531, 66.63468170166016, 66.51708221435547, 66.38613891601562, 66.18281555175781, 66.02129364013672, 65.91011810302734, 65.77119445800781, 65.58161163330078, 65.42424011230469, 65.34405517578125, 65.19652557373047, 65.0594711303711, 64.90287017822266, 64.79199981689453, 64.65435028076172, 64.53612518310547, 64.39215087890625, 64.24159240722656, 64.12207794189453, 64.01785278320312, 63.920291900634766, 63.75813293457031, 63.644527435302734, 63.56235122680664, 63.36075210571289, 63.315555572509766, 63.12281799316406, 63.032840728759766, 62.887569427490234, 62.691436767578125, 62.6665153503418, 62.57984924316406, 62.539363861083984, 62.440189361572266, 62.27516555786133, 62.24360275268555, 62.07948303222656, 62.01632308959961, 61.915069580078125, 61.86891555786133, 61.75865936279297, 61.61371612548828, 61.52387237548828, 61.4183464050293, 61.366310119628906, 61.19921875, 61.14143753051758, 61.02421951293945, 60.999568939208984, 60.86431121826172, 60.78818130493164, 60.74380111694336, 60.65538024902344, 60.67416000366211, 60.635616302490234, 60.50872039794922, 60.3778190612793, 60.2914924621582, 60.15068817138672], 'val_mse': [602.5540161132812, 596.6091918945312, 590.2958374023438, 583.5802612304688, 575.876708984375, 567.3245849609375, 558.3001708984375, 548.70849609375, 538.5706176757812, 527.9337158203125, 516.6656494140625, 504.9603576660156, 492.80426025390625, 480.1930236816406, 467.3052673339844, 454.0971984863281, 440.5169677734375, 426.681640625, 412.9086608886719, 398.7342529296875, 384.5285339355469, 370.34088134765625, 356.1320495605469, 341.8799743652344, 327.8163146972656, 314.066650390625, 300.3447570800781, 286.9599914550781, 273.9934997558594, 261.17498779296875, 249.00277709960938, 237.01156616210938, 225.46783447265625, 214.40296936035156, 203.86477661132812, 193.79757690429688, 184.1313934326172, 175.12033081054688, 166.518798828125, 158.63536071777344, 151.051513671875, 143.86251831054688, 137.52694702148438, 131.61392211914062, 126.2051010131836, 121.2546157836914, 116.48756408691406, 112.41320037841797, 108.75714111328125, 105.34043884277344, 102.32237243652344, 99.52059173583984, 97.11366271972656, 95.08984375, 93.0915298461914, 91.44261169433594, 89.93741607666016, 88.56336975097656, 87.40899658203125, 86.3986587524414, 85.42375946044922, 84.68588256835938, 83.9314956665039, 83.38398742675781, 82.82677459716797, 82.29000091552734, 81.8265151977539, 81.50225067138672, 81.15745544433594, 80.80230712890625, 80.50942993164062, 80.20738220214844, 79.92534637451172, 79.64268493652344, 79.38127136230469, 79.14945983886719, 78.90369415283203, 78.6938247680664, 78.4878921508789, 78.25141906738281, 78.06403350830078, 77.86603546142578, 77.67760467529297, 77.39088439941406, 77.18634033203125, 76.9906005859375, 76.76713562011719, 76.57937622070312, 76.4034652709961, 76.20347595214844, 76.02999877929688, 75.81957244873047, 75.64178466796875, 75.40145111083984, 75.21881103515625, 75.03954315185547, 74.85496520996094, 74.62767028808594, 74.46907043457031, 74.23985290527344, 74.04584503173828, 73.84524536132812, 73.67359161376953, 73.46247863769531, 73.27313995361328, 73.06620788574219, 72.84291076660156, 72.61520385742188, 72.44715881347656, 72.29707336425781, 72.0895767211914, 71.86210632324219, 71.66969299316406, 71.52783203125, 71.31219482421875, 71.0623550415039, 70.92031860351562, 70.71585845947266, 70.55933380126953, 70.37091064453125, 70.23949432373047, 70.0232162475586, 69.90045166015625, 69.67972564697266, 69.4972152709961, 69.33379364013672, 69.07569122314453, 68.92870330810547, 68.77268981933594, 68.62150573730469, 68.44633483886719, 68.25071716308594, 68.04912567138672, 67.97738647460938, 67.79237365722656, 67.60325622558594, 67.4353256225586, 67.30673217773438, 67.1563720703125, 67.03778076171875, 66.82771301269531, 66.63468170166016, 66.51708221435547, 66.38613891601562, 66.18281555175781, 66.02129364013672, 65.91011810302734, 65.77119445800781, 65.58161163330078, 65.42424011230469, 65.34405517578125, 65.19652557373047, 65.0594711303711, 64.90287017822266, 64.79199981689453, 64.65435028076172, 64.53612518310547, 64.39215087890625, 64.24159240722656, 64.12207794189453, 64.01785278320312, 63.920291900634766, 63.75813293457031, 63.644527435302734, 63.56235122680664, 63.36075210571289, 63.315555572509766, 63.12281799316406, 63.032840728759766, 62.887569427490234, 62.691429138183594, 62.6665153503418, 62.57984924316406, 62.539363861083984, 62.440189361572266, 62.27516555786133, 62.24360275268555, 62.07948303222656, 62.01632308959961, 61.915069580078125, 61.86891555786133, 61.75865936279297, 61.61371612548828, 61.52387237548828, 61.4183464050293, 61.366310119628906, 61.19921875, 61.14143753051758, 61.02421951293945, 60.999568939208984, 60.86431121826172, 60.78818130493164, 60.74380111694336, 60.65538024902344, 60.67416000366211, 60.635616302490234, 60.50872039794922, 60.3778190612793, 60.2914924621582, 60.15068817138672]}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1SOkGmAYCRd9",
        "colab_type": "code",
        "outputId": "9c9d6052-0bc4-4e02-c455-bad8506f0738",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "plt.plot(hist['loss'], label='Training Loss')\n",
        "plt.plot(hist['val_loss'], label='Validation Loss')\n",
        "# plt.xticks(np.arange(0, 5), ('1','2','3','4','5'))\n",
        "plt.xlabel('# of epochs')\n",
        "plt.ylabel('MSE')\n",
        "plt.legend();"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mzv9uMLBFKwq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "7fbabb4c-1ecb-4fb5-8d40-07d7f618c001"
      },
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "model = LinearRegression()\n",
        "\n",
        "\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_pred_train = model.predict(X_train)\n",
        "y_pred_test = model.predict(X_test)\n",
        "\n",
        "mean_squared_error(y_train, y_pred_train), mean_squared_error(y_test, y_pred_test)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(22.77423909605731, 18.16551049349646)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "SfcFnOONyuNm"
      },
      "source": [
        "## Use the Keras Library to build an image recognition network using the Fashion-MNIST dataset (also comes with keras)\n",
        "\n",
        "- Load and preprocess the image data similar to how we preprocessed the MNIST data in class.\n",
        "- Make sure to one-hot encode your category labels\n",
        "- The number of nodes in your output layer should equal the number of classes you want to predict for Fashion-MNIST.\n",
        "- Try different hyperparameters. What is the highest accuracy that you are able to achieve.\n",
        "- Use the history object that is returned from model.fit to make graphs of the model's loss or train/validation accuracies by epoch. \n",
        "- Remember that neural networks fall prey to randomness so you may need to run your model multiple times (or use Cross Validation) in order to tell if a change to a hyperparameter is truly producing better results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2xnOs6AjFZLT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        },
        "outputId": "4b3b037b-5a5c-4df2-dc9f-c04a8d97089d"
      },
      "source": [
        "from tensorflow.keras.datasets import fashion_mnist\n",
        "\n",
        "(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "32768/29515 [=================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26427392/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "8192/5148 [===============================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4423680/4422102 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cPLWB1ETFfBO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#using one-hot vector to fit the model\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "y_train = to_categorical(y_train)\n",
        "y_test = to_categorical(y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f8GW_IW7Fqgw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "34a62afa-912e-419f-9945-ba4f209967db"
      },
      "source": [
        "X_train.shape, X_train[0].shape"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((60000, 28, 28), (28, 28))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ppk1GlbXCReM",
        "colab_type": "code",
        "outputId": "68ea2985-9190-417a-89e4-d06102547b5b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 544
        }
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Flatten, Dense\n",
        "model = Sequential([\n",
        "    Flatten(input_shape=(28,28)),\n",
        "    Dense(784, activation=\"relu\"),\n",
        "    Dense(784, activation=\"relu\"),\n",
        "    Dense(784, activation=\"relu\"),\n",
        "    Dense(10, activation=\"softmax\")\n",
        "])\n",
        "\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "model.fit(x=X_train, \n",
        "        y=y_train, \n",
        "        epochs=15, \n",
        "        validation_data=(X_test, y_test))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/15\n",
            "1875/1875 [==============================] - 36s 19ms/step - loss: 1.6254 - accuracy: 0.7897 - val_loss: 0.4789 - val_accuracy: 0.8295\n",
            "Epoch 2/15\n",
            "1875/1875 [==============================] - 35s 19ms/step - loss: 0.4458 - accuracy: 0.8402 - val_loss: 0.5081 - val_accuracy: 0.8156\n",
            "Epoch 3/15\n",
            "1875/1875 [==============================] - 35s 19ms/step - loss: 0.4118 - accuracy: 0.8545 - val_loss: 0.4612 - val_accuracy: 0.8419\n",
            "Epoch 4/15\n",
            "1875/1875 [==============================] - 35s 19ms/step - loss: 0.3994 - accuracy: 0.8582 - val_loss: 0.4170 - val_accuracy: 0.8541\n",
            "Epoch 5/15\n",
            "1875/1875 [==============================] - 35s 19ms/step - loss: 0.3855 - accuracy: 0.8643 - val_loss: 0.4459 - val_accuracy: 0.8517\n",
            "Epoch 6/15\n",
            "1875/1875 [==============================] - 35s 19ms/step - loss: 0.3653 - accuracy: 0.8686 - val_loss: 0.4013 - val_accuracy: 0.8643\n",
            "Epoch 7/15\n",
            "1875/1875 [==============================] - 35s 18ms/step - loss: 0.3522 - accuracy: 0.8769 - val_loss: 0.4564 - val_accuracy: 0.8459\n",
            "Epoch 8/15\n",
            "1875/1875 [==============================] - 35s 19ms/step - loss: 0.3443 - accuracy: 0.8789 - val_loss: 0.4165 - val_accuracy: 0.8571\n",
            "Epoch 9/15\n",
            "1875/1875 [==============================] - 35s 19ms/step - loss: 0.3358 - accuracy: 0.8808 - val_loss: 0.3921 - val_accuracy: 0.8616\n",
            "Epoch 10/15\n",
            "1875/1875 [==============================] - 35s 19ms/step - loss: 0.3261 - accuracy: 0.8847 - val_loss: 0.4632 - val_accuracy: 0.8572\n",
            "Epoch 11/15\n",
            "1875/1875 [==============================] - 34s 18ms/step - loss: 0.3180 - accuracy: 0.8858 - val_loss: 0.4058 - val_accuracy: 0.8607\n",
            "Epoch 12/15\n",
            "1875/1875 [==============================] - 35s 18ms/step - loss: 0.3087 - accuracy: 0.8896 - val_loss: 0.4049 - val_accuracy: 0.8670\n",
            "Epoch 13/15\n",
            "1875/1875 [==============================] - 35s 19ms/step - loss: 0.3127 - accuracy: 0.8875 - val_loss: 0.3937 - val_accuracy: 0.8654\n",
            "Epoch 14/15\n",
            "1875/1875 [==============================] - 35s 19ms/step - loss: 0.3092 - accuracy: 0.8911 - val_loss: 0.4329 - val_accuracy: 0.8616\n",
            "Epoch 15/15\n",
            "1875/1875 [==============================] - 35s 18ms/step - loss: 0.2998 - accuracy: 0.8935 - val_loss: 0.3934 - val_accuracy: 0.8713\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f13a86ac400>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kNN-3NbqGqWs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "hist1 = model.history.history"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BYleA0CDCReR",
        "colab_type": "code",
        "outputId": "0414dbbb-d9d5-4f3b-d107-852b7cb0107d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        }
      },
      "source": [
        "plt.plot(hist1['loss'], label='Training Loss')\n",
        "plt.plot(hist1['val_loss'], label='Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('MSE')\n",
        "plt.legend();"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zv_3xNMjzdLI"
      },
      "source": [
        "## Stretch Goals:\n",
        "\n",
        "- Use Hyperparameter Tuning to make the accuracy of your models as high as possible. (error as low as possible)\n",
        "- Use Cross Validation techniques to get more consistent results with your model.\n",
        "- Use GridSearchCV to try different combinations of hyperparameters. \n",
        "- Start looking into other types of Keras layers for CNNs and RNNs maybe try and build a CNN model for fashion-MNIST to see how the results compare."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YKVVdthg3AE7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}